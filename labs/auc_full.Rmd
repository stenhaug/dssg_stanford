---
title: "auc_student_version"
author: "E Flynn"
date: "8/6/2019"
output: html_document
---
## Understanding Binary Predictions

```{r}
library(tidyverse)

```


### Calculating and understanding AUROC curves

Let's generate data - all we need to make is the outcome and a proba score from a model. You might imagine the outcome is getting the flu this season.

```{r}
flu_data <- 
    tibble(proba = runif(1000)) %>%  # sample from a uniform distribution, proba is the probability
    mutate(outcome = as.integer(proba > runif(1000))) %>%  # this creates an outcome with some randomness in the value
    select(proba, outcome)
```


When we create a model, we aren't just outputting probabilities, we want to consider at what probability cutoff we label something a certain way. So if the model output flu probability of 0.9 - do we think someone will get flu? What about 0.6?

To do this, we can select a cutoff. Let's set it to 0.5 - so above 0.5 is predicted flu, below or equal is predicted not flu. 

1. Add a column labeled "pred" that contains the prediction for each row. (Use `mutate`!)
```{r}

```

2. Now count the number of true positives, false positives, false negatives, and true negatives. (Use `summarize`!)
```{r}

```

3. Use this to calculate the accuracy, sensitivity, specificity, precision, and recall. 
```{r}

```

#### Plotting AUC
4. Now you could imagine taking different cutoffs and getting different proportions of FP, FN, and specificities and sensitivities, etc.
Below is a function to calculate fp/fn/tp/fn, specificity, sensitivity, and accuracy for any cutoff. 
Look at the code for this. Make sure you understand how it works!  (Hint - try pieces of it.)
  - Add comments at the hashtags.
  - What happens as the cutoff increases?
  - What happens as the cutoff decreases?
```{r}
cuts <- seq(0,1,length.out=1000) # TODO: comment

EPSILON <- 0.0001 # so that we don't get 0 in the denom
acc_counts <- function(dat){
  dat %>% 
    summarize( # TODO: comment
      tp=sum(pred==1 & outcome==1),
      fp=sum(pred==1 & outcome==0),
      fn=sum(pred==0 & outcome==1),
      tn=sum(pred==0 & outcome==0)) %>% 
    mutate( # TODO: comment
      acc = (tp+tn)/(tp+tn+fp+fn),
      sens = tp/(tp+fn+EPSILON), # == true positive rate, recall
      spec = tn/(tn+fp+EPSILON),
      prec = tp/(tp+fp+EPSILON),
      recall = tp/(tp+fn+EPSILON)
   )
}
calc_acc_metrics <- function(cut, dat){
  dat %>% 
    mutate(pred=ifelse(proba > cut, 1, 0)) %>% # 
    acc_counts()
}

# TODO: comment
flu_cut_auc <- cuts %>% map(calc_acc_metrics, flu_data) %>% bind_rows()

```

Using this we can create an AUROC curve.
```{r}
ggplot(flu_cut_auc, aes(x=(1-spec), y=sens))+ geom_point() + geom_line()
```

This curve describes how well the model separates the data.

5. Which point would you pick as a cutoff for your model? Why? What might make you change your mind?
  - What is the number of FP when sensitivity is 0.8?
  - What is the number of FN when specificity is 0.8?
  - (What is the cut point that corresponds to these?)


6. What if we guessed randomly? or perfectly? Look at the output of these two and describe. 
```{r}
# PERFECT
flu_perf <- flu_data %>%
    mutate(pred=outcome) %>%
    acc_counts()

# RANDOM
flu_rand <- flu_data %>%
    mutate(pred=as.integer(runif(1000))) %>%
    acc_counts()

```

For the above two predictions, the outcomes do not vary by cutpoint. 
To plot these on an AUC plot, we add the extremes (0,1) and (1,0)

```{r}
add_auc_end_pts <- function(dat){
  dat2 <- dat %>% select(spec, sens)
  dat3 <- do.call(rbind, list(dat2, c("spec"=0,"sens"=1), c("spec"=1,"sens"=0)))
  dat4 <- dat3 %>% arrange(spec, sens) 
  return(dat4)
}
```


Now, let's plot all three together. What do you notice?
```{r}
# add a endpoints and a column to describe each
flu_rand2 <- add_auc_end_pts(flu_rand)
flu_perf2 <- add_auc_end_pts(flu_perf)
flu_rand2$type <- "rand"
flu_perf2$type <- "perf"
flu_cut_auc2 <- flu_cut_auc %>% select(spec, sens)
flu_cut_auc2$type <- "m1"

# put them together
all_dat <- do.call(rbind, list(flu_rand2, flu_perf2, flu_cut_auc2))

# plot
ggplot(all_dat, aes(x=(1-spec), y=sens, color=type))+ geom_point() + geom_line()

```


7. We can also do this using the AUC package (you may need to install it).
Look up the documentation for this and create the same plot below. 
Then calculate the area under the curve value using `auc`.
```{r}
# install.packages("AUC")
library(AUC)
```


#### Separability
Now there are lots of different cutoffs we could choose - and looking at the separability of these cutoffs is how we figure out how well the model separates the outcomes.

In order to understand this, it's important to think through the separability of the outcomes. 
Let's look at the distributions of the original data. We can add a line at a particular cutoff - such as 0.5.

8. Which areas are the TP, FP, FN, and FP?
```{r}
flu_data <- flu_data %>% mutate(
  outcome=as.factor(outcome)
)
ggplot(flu_data, aes(x=proba, fill=outcome)) +
  geom_density(alpha=0.4) + 
  geom_vline(aes(xintercept=0.5), linetype="dashed")
```


9. Look at the two plots below - if your data looks like this, what do you expect to see for AUROC? What do you expect the AUCs to be? 
```{r}
alt_data1 <-
  tibble(proba = c(rnorm(500, 0.3, 0.05), rnorm(500, 0.8, 0.05))) %>%  # sample from two separated normal distributions
    mutate(outcome = c(rep(0, 500), rep(1, 500))) %>% # label according to which was sampled from
    select(proba, outcome) %>%
  mutate(outcome=as.factor(outcome))

ggplot(alt_data1, aes(x=proba, fill=outcome)) +
  geom_density(alpha=0.4) 
```

```{r}
alt_data2 <-
  tibble(proba = runif(1000)) %>%  # sample from a uniform distribution, proba is the probability
    mutate(outcome = as.integer(0.5 < runif(1000))) %>%  # label it randomly
    select(proba, outcome) %>%
  mutate(outcome=as.factor(outcome))

ggplot(alt_data2, aes(x=proba, fill=outcome)) +
  geom_density(alpha=0.4) 
```


### Model comparison

AUROC curves are frequently used in the literature to compare different models.

10. Let's take the "model" we had before and create another model - how do the two compare. Which model would you pick? Why?

```{r}
flu_data_m2 <- 
    tibble(proba = runif(1000)) %>%  # sample from a uniform distribution, proba is the probability
    mutate(outcome = as.integer((proba + runif(1000)) > runif(1000))) %>%  # this creates an outcome with some randomness in the value
    select(proba, outcome)
flu_cut_m2 <- cuts %>% map(calc_acc_metrics, flu_data_m2) %>% bind_rows()

flu_cut_m2.2 <- flu_cut_m2 %>% select(spec, sens)
flu_cut_m2.2$type <- "m2"

# put them together
all_dat2 <- rbind(all_dat, flu_cut_m2.2)

# plot
ggplot(all_dat2, aes(x=(1-spec), y=sens, color=type))+ geom_point() + geom_line()
```


### AUPRC curves 
While AUROC is the most common way to look at this, information retrieval relies on precision and recall. Precision is the number of true positives out of those labeled positive, while recall is the same as sensitivity - the number of positives we label out of the total number of positives. 
The precision recall curve looks a little different but is very helpful. 

It is just like the AUC plot but with precision and recall as the axes. 

11. Use the `flu_cut_auc` object and plot precision (y-axis) vs. recall (x-axis) using ggplot. 
```{r}


```


The "no-skill" line for a precision-recall curve is located at the number of positives/total. We can think of this as if the model always predicts positive. 

12. Where should this line be drawn for the above model? Plot it.
```{r}

```

### Ben's brilliant representation :)
Ben came up with a great representation to show precision, accuracy, and recall.

13. Run his code - what does this look like. Why is this helpful?
```{r}
source("ben_acc_pr.R")
```



#### STORM outcome
18. Look at the AUC plot (Fig 2) in the STORM model we examined on Friday. Given that only 13k patients had an overdose or suicide attempt and the whole dataset is 1.2mil patients - what should you be concerned about when looking at this plot? Discuss. 
