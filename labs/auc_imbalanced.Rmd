---
title: "auc_imbalanced.Rmd"
author: "E Flynn"
date: "8/6/2019"
output: html_document
---

```{r}
require('tidyverse')
```

### AUC intro

Let's generate data - all we need to make is the outcome and a proba score from a model. You might imagine the outcome is getting the flu this season.

```{r}
flu_data <- 
    tibble(proba = runif(1000)) %>%  # sample from a uniform distribution, proba is the probability
    mutate(outcome = as.integer(proba > runif(1000))) %>%  # this creates an outcome with some randomness in the value
    select(proba, outcome)
```

When we create a model, we aren't just outputting probabilities, we want to consider at what probability cutoff we label something a certain way. So if the model output flu probability of 0.9 - do we think someone will get flu? What about 0.6?

To do this, we can select a cutoff. Let's set it to 0.5 - so above 0.5 is predicted flu, below or equal is predicted not flu. 

#### Plotting AUC
1. Now you could imagine taking different cutoffs and getting different proportions of FP, FN, and specificities and sensitivities, etc.
Below is a function to calculate fp/fn/tp/fn, specificity, sensitivity, and accuracy for any cutoff. 
Look at the code for this. Make sure you understand how it works!  (Hint - try pieces of it.)
  - Add comments at the hashtags.
  - What happens as the cutoff increases?
  - What happens as the cutoff decreases?
```{r}
cuts <- seq(0,1,length.out=1000) # TODO: comment

EPSILON <- 0.0001 # so that we don't get 0 in the denom
acc_counts <- function(dat){
  dat %>% 
    summarize( # TODO: comment
      tp=sum(pred==1 & outcome==1),
      fp=sum(pred==1 & outcome==0),
      fn=sum(pred==0 & outcome==1),
      tn=sum(pred==0 & outcome==0)) %>% 
    mutate( # TODO: comment
      acc = (tp+tn)/(tp+tn+fp+fn),
      sens = tp/(tp+fn+EPSILON), # == true positive rate, recall
      spec = tn/(tn+fp+EPSILON),
      prec = tp/(tp+fp+EPSILON),
      recall = tp/(tp+fn+EPSILON)
   )
}
calc_acc_metrics <- function(cut, dat){
  dat %>% 
    mutate(pred=ifelse(proba > cut, 1, 0)) %>% # 
    acc_counts()
}

# TODO: comment
flu_cut_auc <- cuts %>% map(calc_acc_metrics, flu_data) %>% bind_rows()

```

Using this we can create an AUROC curve.
```{r}
ggplot(flu_cut_auc, aes(x=(1-spec), y=sens))+ geom_point() + geom_line()
```


```{r}
flu_cut_auc$cut <- cuts
flu_cut_auc %>% 
  filter(acc ==max(acc))

```

Which point would you pick as a cutoff for your model? Why? What might make you change your mind?
  - What is the number of FP when sensitivity is 0.8?
  - What is the number of FN when specificity is 0.8?
  - (What is the cut point that corresponds to these?)


3. What if we guessed randomly? or perfectly? Look at the output of these two and describe. 
```{r}
# PERFECT
flu_perf <- flu_data %>%
    mutate(pred=outcome) %>%
    acc_counts()

# RANDOM
flu_rand <- flu_data %>%
    mutate(pred=as.integer(runif(1000))) %>%
    acc_counts()

```

For the above two predictions, the outcomes do not vary by cutpoint. 
To plot these on an AUC plot, we add the extremes (0,1) and (1,0)

```{r}
add_auc_end_pts <- function(dat){
  dat2 <- dat %>% select(spec, sens)
  dat3 <- do.call(rbind, list(dat2, c("spec"=0,"sens"=1), c("spec"=1,"sens"=0)))
  dat4 <- dat3 %>% arrange(spec, sens) 
  return(dat4)
}
```


Now, let's plot all three together. What do you notice?
```{r}
# add a endpoints and a column to describe each
flu_rand2 <- add_auc_end_pts(flu_rand)
flu_perf2 <- add_auc_end_pts(flu_perf)
flu_rand2$type <- "rand"
flu_perf2$type <- "perf"
flu_cut_auc2 <- flu_cut_auc %>% select(spec, sens)
flu_cut_auc2$type <- "m1"

# put them together
all_dat <- do.call(rbind, list(flu_rand2, flu_perf2, flu_cut_auc2))

# plot
ggplot(all_dat, aes(x=(1-spec), y=sens, color=type))+ geom_point() + geom_line()

```


#### Separability
One way of thinking of the AUC curve is that it describes how well the model separates the data. But we also need to consider whether the data is separable.

4. Look at the three plots below - how well is the data separated? Which areas are the TP, FP, FN, and FP? What do you expect to see for AUROC? What do you expect the AUCs to be? 

Now there are lots of different cutoffs we could choose - and looking at the separability of these cutoffs is how we figure out how well the model separates the outcomes.

```{r}
# original data
flu_data <- flu_data %>% mutate(
  outcome=as.factor(outcome)
)
ggplot(flu_data, aes(x=proba, fill=outcome)) +
  geom_density(alpha=0.4) + 
  geom_vline(aes(xintercept=0.5), linetype="dashed") # draw a line at cutoff = 0.5
```

```{r}
alt_data1 <-
  tibble(proba = c(rnorm(500, 0.3, 0.05), rnorm(500, 0.8, 0.05))) %>%  # sample from two separated normal distributions
    mutate(outcome = c(rep(0, 500), rep(1, 500))) %>% # label according to which was sampled from
    select(proba, outcome) %>%
  mutate(outcome=as.factor(outcome))

ggplot(alt_data1, aes(x=proba, fill=outcome)) +
  geom_density(alpha=0.4) 
```

```{r}
alt_data2 <-
  tibble(proba = runif(1000)) %>%  # sample from a uniform distribution, proba is the probability
    mutate(outcome = as.integer(0.5 < runif(1000))) %>%  # label it randomly
    select(proba, outcome) %>%
  mutate(outcome=as.factor(outcome))

ggplot(alt_data2, aes(x=proba, fill=outcome)) +
  geom_density(alpha=0.4) 
```



#### Precision-Recall
While AUROC is the most common way to look at model performance, information retrieval relies on precision and recall. Precision is the number of true positives out of those labeled positive, while recall is the same as sensitivity - the number of positives we label out of the total number of positives. 
The precision recall curve looks a little different but is very helpful. 

```{r}
flu_cut_pr <- filter(flu_cut_auc, prec != 0) # remove the 0,0 point so we can plot properly
ggplot(flu_cut_pr, aes(x=recall, y=prec))+geom_point()+geom_line()

```


The "no-skill" line for a precision-recall curve is located at the number of positives/total. We can think of this as if the model always predicts positive. 

4. Where should this line be drawn for the above model?  How does this plot look different from the AUC plot? 


### Ben's excellent representation :)

5. Run his code - what does this look like? Let's discuss. 
Why is this helpful?
What happens to precision, recall, and accuracy as the number treated increases? 
At each end?
```{r}
source("ben_acc_pr.R")
```


### Imbalanced classes
Before our breakdown was about 50-50; the reality though is that most people do not get flu. If we switch it to 90-10 (10\% get flu), what happens? For most of the problems we are dealing with, our datasets are quite imbalanced!

New data with approx 90-10 breakdown.
```{r}
flu_data_imb <- 
    tibble(proba = runif(1000)) %>%  # sample from a uniform distribution, proba is the probability
    mutate(outcome = as.integer(proba > 0.5+runif(1000))) %>% 
    select(proba, outcome)

table(flu_data$outcome) # what was the breakdown before?

table(flu_data_imb$outcome) # what is the breakdown now?
```

6. Plot and calculate the AUC. How does this look? How good do you feel about this model?
```{r}
imb_counts<- cuts %>% map(calc_acc_metrics, flu_data_imb) %>% bind_rows() 
ggplot(flu_cut_auc, aes(x=(1-spec), y=sens))+ geom_point() + geom_line()

```

What if it just so happens that we build a model that realizes there are more no flu outcomes than flu outcomes and decides to always predict that people do not have flu. 

7. How does this compare to the "model" we have before? Calculate maximum accuracy of the "model" above - how does this compare to the accuracy max-class model below? What do you think of the original model now?
```{r}
imb_max_class <- flu_data_imb %>%
    mutate(pred=0) %>%
    acc_counts()
```

8. Plot the precision-recall curve for the imbalanced data. Is this helpful? Why? Describe what happens in this plot. 
```{r}

```

We can see this drops very fast and is quite low. For data with imbalanced classes, precision-recall tends to provide a much better picture of what is going on. 

9. What is the precision and recall of the max-class model?
What happens if you swap the classes? (e.g. flu is 0, no-flu is 1, and the no-skill model predicts 1)? Do this. What do we need to be careful about then when looking at precision-recall curves?
```{r}


```

#### STORM outcome
10. Look at the AUC plot (Fig 2) in the STORM model we examined on Friday. Given that only 13k patients had an overdose or suicide attempt and the whole dataset is 1.2mil patients - what should you be concerned about when looking at this plot? Discuss. 
